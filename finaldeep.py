# -*- coding: utf-8 -*-
"""FinalDeep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XVNxR5S4LVVh64sV8SKJGW_eGcnOkLCG

SECCIÃ“N 1 â€” InstalaciÃ³n de dependencias
"""

!pip install librosa gdown soundfile tensorflow_hub

"""SECCIÃ“N 2 â€” Importaciones y ConfiguraciÃ³n del Dataset"""

import numpy as np
import librosa
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow_hub as hub
import soundfile as sf
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
import gdown
import zipfile

"""SECCIÃ“N 3 â€” Descarga y extracciÃ³n del Dataset
ðŸ“Œ Se organiza automÃ¡ticamente en carpetas por gÃ©nero
"""

# ID del archivo GTZAN en tu Google Drive
file_id = "1XP20Z-T-Q_7uvyXjPS7-aIxW_8XRhYo9"
url = f"https://drive.google.com/uc?id={file_id}"

output_zip = "/content/genres.zip"

print("ðŸ“¥ Descargando dataset...")
gdown.download(url, output_zip, quiet=False)

print("ðŸ“‚ Extrayendo...")
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall('/content/')

DATA_PATH = "/content/genres"

GENRES = ['blues','classical','country','disco','hiphop',
          'jazz','metal','pop','reggae','rock']

"""SECCIÃ“N 4 â€” Modelo YAMNet para extraer embeddings

ðŸ“Œ Convierte audio â†’ vector 1024 dimensiones
"""

yamnet = hub.load('https://tfhub.dev/google/yamnet/1')

def extract_yamnet_embeddings(file_path):
    try:
        audio, sr = sf.read(file_path, dtype='float32')

        if sr != 16000:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)

        _, embeddings, _ = yamnet(audio)
        return np.mean(embeddings.numpy(), axis=0)

    except Exception as e:
        print("Error:", e)
        return None

"""SECCIÃ“N 5 â€” Carga del dataset y extracciÃ³n de embeddings"""

print("ðŸŽ§ Extrayendo embeddings...")

features = []
labels = []

for genre in GENRES:
    folder = os.path.join(DATA_PATH, genre)
    files = [f for f in os.listdir(folder) if f.endswith(".au")]

    for f in files:
        fp = os.path.join(folder, f)
        emb = extract_yamnet_embeddings(fp)

        if emb is not None:
            features.append(emb)
            labels.append(genre)

X = np.array(features)
y = np.array(labels)

print("âœ” Listo!")
print("X shape:", X.shape)
print("Samples:", len(y))

"""SECCIÃ“N 6 â€” CodificaciÃ³n y DivisiÃ³n Train/Test"""

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_cat = to_categorical(y_encoded)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_cat, test_size=0.2, random_state=42, stratify=y_encoded
)

print("ðŸ“Š Train:", X_train.shape, "| Test:", X_test.shape)

"""SECCIÃ“N 7 â€” Modelo de ClasificaciÃ³n

ðŸ“Œ Entrada: 1024 features â†’ Salida: 10 gÃ©neros
"""

model = Sequential([
    Dense(512, activation='relu', input_shape=(1024,)),
    Dropout(0.3),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(len(GENRES), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""SECCIÃ“N 8 â€” Entrenamiento del modelo"""

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=40,
    batch_size=32,
    verbose=1
)

"""SECCIÃ“N 9 â€” EvaluaciÃ³n"""

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nðŸŽ¯ Accuracy en test: {acc*100:.2f}%")

"""SECCIÃ“N 10 â€” PredicciÃ³n desde audio real + Reproductor"""

import ipywidgets as widgets
from IPython.display import Audio, display, clear_output
import random

def test_random_audio(_=None):
    clear_output(wait=True)
    print("ðŸŽ² Seleccionando audio aleatorio...")

    genre = random.choice(GENRES)
    folder = os.path.join(DATA_PATH, genre)
    files = [f for f in os.listdir(folder) if f.endswith(".au")]

    file = random.choice(files)
    fp = os.path.join(folder, file)

    print(f"\nðŸ—‚ Archivo: {file}")
    print(f"ðŸŽµ GÃ©nero real: {genre}")

    # Reproducir audio
    audio, sr = librosa.load(fp, sr=None)
    display(Audio(audio, rate=sr))

    emb = extract_yamnet_embeddings(fp)
    emb = emb.reshape(1, -1)

    pred = model.predict(emb, verbose=0)[0]
    idx = np.argmax(pred)
    pred_genre = label_encoder.inverse_transform([idx])[0]

    print(f"\nðŸ”® PredicciÃ³n: {pred_genre}")
    print("-----------------------------------")

    button = widgets.Button(description="â–¶ Otro audio", button_style="success")
    button.on_click(test_random_audio)
    display(button)

test_random_audio()